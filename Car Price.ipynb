{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#reading the dataset\n",
    "data=pd.read_csv('car data.csv')\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "print('Unique elements in Seller_Type are',data['Seller_Type'].unique())\n",
    "print('Unique elements in Fuel_Type are',data['Fuel_Type'].unique())\n",
    "print('Unique elements in Transmission are',data['Transmission'].unique())\n",
    "print('Unique elements in Owner are',data['Owner'].unique())\n",
    "print('Unique elements in Year are',data['Year'].unique())\n",
    "\n",
    "print('Unique elements in Car_Name are',data['Car_Name'].nunique())\n",
    "#98 unique elements \n",
    "#so, rather than encoding it, we can just drop this columbn as it doesn' make sense\n",
    "\n",
    "data.describe()\n",
    "\n",
    "dataset=data[['Year','Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission','Owner']]\n",
    "dataset.head()\n",
    "\n",
    "dataset['Present_Year']=2020\n",
    "dataset['Number_of_Years_Old']=dataset['Present_Year']- dataset['Year']\n",
    "dataset.head()\n",
    "\n",
    "dataset.drop(labels=['Year', 'Present_Year'],axis=1,inplace=True)\n",
    "dataset.head()\n",
    "\n",
    "#select categorical variables from then dataset, and then implement categorical encoding for nominal variables\n",
    "Fuel_Type=dataset[['Fuel_Type']]\n",
    "Fuel_Type=pd.get_dummies(Fuel_Type, drop_first=True)\n",
    "\n",
    "Seller_Type=dataset[['Seller_Type']]\n",
    "Seller_Type=pd.get_dummies(Seller_Type, drop_first=True)\n",
    "\n",
    "Transmission=dataset[['Transmission']]\n",
    "Transmission=pd.get_dummies(Transmission, drop_first=True)\n",
    "\n",
    "dataset=pd.concat([dataset,Fuel_Type, Seller_Type, Transmission], axis=1)\n",
    "\n",
    "dataset.drop(labels=['Fuel_Type', 'Seller_Type', 'Transmission'], axis=1, inplace=True)\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "dataset.columns\n",
    "\n",
    "# Dataset Correlation\n",
    "dataset.corr()\n",
    "\n",
    "#Correlations of features in dataset\n",
    "corrmat = data.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(10,10))\n",
    "#Plot heat map\n",
    "sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n",
    "sell=dataset['Selling_Price']\n",
    "dataset.drop(['Selling_Price'], axis=1, inplace=True)\n",
    "dataset=dataset.join(sell)\n",
    "dataset.head()\n",
    "\n",
    "X=dataset.iloc[:,:-1]\n",
    "y=dataset.iloc[:,-1]\n",
    "\n",
    "### To determine important features, make use of ExtraTreesRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model = ExtraTreesRegressor()\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.feature_importances_)\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "X=dataset.iloc[:,:-1].values\n",
    "y=dataset.iloc[:,-1].values\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#kfold = StratifiedKFold(n_splits=3)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0\n",
    "                                                    \n",
    "#Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt_reg = DecisionTreeRegressor(random_state = 0)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred=dt_reg.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Score on Training set is\",dt_reg.score(X_train, y_train))#Training Accuracy\n",
    "print(\"Decision Tree Score on Test Set is\",dt_reg.score(X_test, y_test))#Testing Accuracy\n",
    "\n",
    "accuracies = cross_val_score(dt_reg, X_train, y_train, cv = 5)\n",
    "print(accuracies)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "mae=mean_absolute_error(y_pred, y_test)\n",
    "print(\"Mean Absolute Error:\" , mae)\n",
    "\n",
    "mse=mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\" , mse)\n",
    "\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "print('The r2_score is', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "sns.distplot(y_test-y_pred)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha = 0.5)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()\n",
    "                                                    \n",
    "#Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=400,min_samples_split=15,min_samples_leaf=2,\n",
    "max_features='auto', max_depth=30)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred=rf_reg.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Score on Training set is\",rf_reg.score(X_train, y_train))#Training Accuracy\n",
    "print(\"Random Forest Score on Test Set is\",rf_reg.score(X_test, y_test))#Testing Accuracy\n",
    "\n",
    "accuracies = cross_val_score(rf_reg, X_train, y_train, cv = 5)\n",
    "print(accuracies)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "mae=mean_absolute_error(y_pred, y_test)\n",
    "print(\"Mean Absolute Error:\" , mae)\n",
    "\n",
    "mse=mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\" , mse)\n",
    "\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "print('The r2_score is', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "sns.distplot(y_test-y_pred)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha = 0.5)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()\n",
    "                                                    \n",
    "                                                    from sklearn.ensemble import VotingRegressor\n",
    "vot_reg = VotingRegressor([('DecisionTree', dt_reg), ('RandomForestRegressor', rf_reg)])\n",
    "vot_reg.fit(X_train, y_train)\n",
    "y_pred=vot_reg.predict(X_test)\n",
    "\n",
    "print(\"Voting Regresssor Score on Training set is\",vot_reg.score(X_train, y_train))#Training Accuracy\n",
    "print(\"Voting Regresssor Score on Test Set is\",vot_reg.score(X_test, y_test))#Testing Accuracy\n",
    "\n",
    "accuracies = cross_val_score(vot_reg, X_train, y_train, cv = 5)\n",
    "print(accuracies)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "mae=mean_absolute_error(y_pred, y_test)\n",
    "print(\"Mean Absolute Error:\" , mae)\n",
    "\n",
    "mse=mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\" , mse)\n",
    "\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "print('The r2_score is', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "sns.distplot(y_test-y_pred)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha = 0.5)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()\n",
    "                                                    \n",
    "import pickle\n",
    "pickle.dump(vot_reg, open(\"vot_reg.pkl\", \"wb\"))\n",
    "\n",
    "# load model from file\n",
    "model = pickle.load(open(\"vot_reg.pkl\", \"rb\"))\n",
    "\n",
    "model.predict([[9.85, 6900, 0, 3, 0, 1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11298385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
